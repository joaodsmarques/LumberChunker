{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Loading HuggingFace Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# For Loading Retriever Model and Evaluation\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "model = AutoModel.from_pretrained('facebook/contriever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(\"hf://datasets/LumberChunker/GutenQA/GutenQA.parquet\", engine=\"pyarrow\")\n",
    "questions = pd.read_parquet(\"hf://datasets/LumberChunker/GutenQA/questions.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_name = \"A_Christmas_Carol_-_Charles_Dickens\"\n",
    "\n",
    "# Filter the Chunks DataFrame to show only rows with the specified book name\n",
    "single_book_chunks = dataset[dataset['Book Name'] == book_name].reset_index(drop=True)\n",
    "\n",
    "# Filter the Questions DataFrame to show only the generated questions for the target book.\n",
    "single_book_qa = questions[questions['Book Name'] == book_name].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling for Embeddings\n",
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "# Apply tokenizer to book chunks and questions\n",
    "inputs_chunks = tokenizer(single_book_chunks[\"Chunk\"].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "inputs_questions = tokenizer(single_book_qa[\"Question\"].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "outputs_chunks = model(**inputs_chunks)\n",
    "outputs_questions = model(**inputs_questions)\n",
    "\n",
    "embeddings_chunks = mean_pooling(outputs_chunks[0], inputs_chunks['attention_mask']).detach().cpu().numpy()\n",
    "embeddings_questions = mean_pooling(outputs_questions[0], inputs_questions['attention_mask']).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold label is the substring that is present on the Chunk_Must_Contain column.\n",
    "# We look if that substring is present on the retrieved chunks. \n",
    "# If it is, that index position on 'relevance' list receives the value = 1 and the remaining positions 0.\n",
    "def find_index_of_match(answers, gold_label):\n",
    "    relevance = []\n",
    "    gold_label = gold_label.lower()\n",
    "    for _, item in enumerate(answers):\n",
    "        if gold_label in item.lower():\n",
    "            relevance.append(1)\n",
    "            relevance = relevance + ((len(answers) - len(relevance))* ([0]))\n",
    "            break\n",
    "        else:\n",
    "            relevance.append(0)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "\n",
    "def compute_DCG(rel):\n",
    "    aux = 0\n",
    "    for i in range(1, len(rel)+1):\n",
    "        aux = aux + (np.power(2,rel[i-1])-1) / (np.log2(i+1))\n",
    "    return(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(top_k, query_individual_embedding_numpy):\n",
    "    similarity = np.dot(embeddings_chunks, np.transpose(query_individual_embedding_numpy))\n",
    "    top_indices = np.argsort(similarity, axis=0)[-top_k:]\n",
    "    top_indices = top_indices[::-1]\n",
    "\n",
    "    answers = []\n",
    "\n",
    "    for i in range(len(top_indices)):\n",
    "        answers.append(single_book_chunks.at[top_indices[i], 'Chunk'])\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to calculate DCG@k for k between 1 and 20\n",
    "DCG_k_sweep = []\n",
    "for j in [1, 2, 5, 10, 20]:\n",
    "    DCG_list = []\n",
    "\n",
    "    for k in range(len(single_book_qa)):\n",
    "        query_embedding = embeddings_questions[k]\n",
    "        answers = get_top_k(top_k = j, query_individual_embedding_numpy= embeddings_questions[k])\n",
    "        gold_label = single_book_qa.loc[k, \"Chunk Must Contain\"]\n",
    "        rel = find_index_of_match(answers=answers, gold_label=gold_label)\n",
    "        DCG_list.append(compute_DCG(rel))\n",
    "\n",
    "    DCG_k_sweep.append(np.mean(DCG_list))\n",
    "\n",
    "# Print the DCG_k_sweep list\n",
    "print(DCG_k_sweep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac6930534f871d314aca2610c2357ec063ba4065ca1d2a97333736987f270c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
