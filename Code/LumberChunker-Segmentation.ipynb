{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, GenerationConfig\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import initializer as aiplatform_initializer\n",
    "from google.cloud.aiplatform_v1beta1 import types as aiplatform_types\n",
    "from google.cloud.aiplatform_v1beta1.services import prediction_service\n",
    "from google.cloud.aiplatform_v1beta1.types import (\n",
    "    content as gapic_content_types,\n",
    ")\n",
    "from google.cloud.aiplatform_v1beta1.types import (\n",
    "    prediction_service as gapic_prediction_service_types,\n",
    ")\n",
    "from google.cloud.aiplatform_v1beta1.types import tool as gapic_tool_types\n",
    "from vertexai.language_models import (\n",
    "    _language_models as tunable_models,\n",
    ")\n",
    "\n",
    "project_id = \"affine-oauth\"\n",
    "location = \"europe-west3\"\n",
    "vertexai.init(project = project_id, location = location)\n",
    "model = GenerativeModel('gemini-pro')\n",
    "\n",
    "\n",
    "HarmCategory = gapic_content_types.HarmCategory\n",
    "HarmBlockThreshold = gapic_content_types.SafetySetting.HarmBlockThreshold\n",
    "FinishReason = gapic_content_types.Candidate.FinishReason\n",
    "SafetyRating = gapic_content_types.SafetyRating\n",
    "\n",
    "model_type = \"Gemini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"insert OpenAI key here\")\n",
    "model_type = \"ChatGPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(input_string):\n",
    "    words = input_string.split()\n",
    "    return round(1.2*len(words))\n",
    "\n",
    "\n",
    "# Function to add IDs\n",
    "def add_ids(row):\n",
    "    global current_id\n",
    "    # Add ID to the chunk\n",
    "    row['Chunk'] = f'ID {current_id}: {row[\"Chunk\"]}'\n",
    "    current_id += 1\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"You will receive as input an english document with paragraphs identified by 'ID XXXX: <text>'.\n",
    "\n",
    "Task: Find the first paragraph (not the first one) where the content clearly changes compared to the previous paragraphs.\n",
    "\n",
    "Output: Return the ID of the paragraph with the content shift as in the exemplified format: 'Answer: ID XXXX'.\n",
    "\n",
    "Additional Considerations: Avoid very long groups of paragraphs. Aim for a good balance between identifying content shifts and keeping groups manageable.\"\"\"\n",
    "\n",
    "\n",
    "def LLM_prompt(model_type, user_prompt):\n",
    "   \n",
    "\n",
    "    if model_type == \"Gemini\":\n",
    "        GenerationConfig = {\"temperature\": 0.1}\n",
    "        while True:\n",
    "            try:\n",
    "                response = model.generate_content(\n",
    "                contents = user_prompt,\n",
    "                generation_config = GenerationConfig,\n",
    "                #We want to avoid as possible the model refusing the query, hence we set the BlockThresholds to None.\n",
    "                safety_settings={\n",
    "                                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                            })\n",
    "\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            except Exception as e:\n",
    "                # For scenarios where the model still blocks content, we can't redo the prompt, or it will block again. Hence we will increment the ID of the 1st chunk.\n",
    "                if str(e) == \"list index out of range\":\n",
    "                    print(\"Gemini thinks prompt is unsafe\")\n",
    "                    return \"content_flag_increment\"\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}. Retrying in 1 minute...\")\n",
    "                    time.sleep(60)  # Wait for 1 minute before retrying\n",
    "    \n",
    "\n",
    "    elif model_type == \"ChatGPT\":\n",
    "        while True:\n",
    "            try:\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo-0125\",\n",
    "                    temperature=0.1,\n",
    "                    messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                    ],)\n",
    "                return completion.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                if str(e) == \"list index out of range\":\n",
    "                    print(\"GPT thinks prompt is unsafe\")\n",
    "                    return \"content_flag_increment\"\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}. Retrying in 1 minute...\")\n",
    "                    time.sleep(60)  # Wait for 1 minute before retrying\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/avduarte333/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_parquet(\"hf://datasets/LumberChunker/GutenQA_Paragraphs/GutenQA_paragraphs.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"insert path to save files here\"\n",
    "\n",
    "# Insert Book Name Here (We choose A Christmas Carol by Charles_Dickens for the Example)\n",
    "book_name = \"A_Christmas_Carol_-_Charles_Dickens\"\n",
    "fileOut  = f'{path}/Gemini_Chunks_-_{book_name}.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "# Filter the DataFrame to show only rows with the specified book name\n",
    "paragraph_chunks = dataset[dataset['Book Name'] == book_name].reset_index(drop=True)\n",
    "id_chunks = paragraph_chunks['Chunk'].to_frame()\n",
    "\n",
    "\n",
    "# Initialize a global variable for current_id and Apply the function along the rows of the DataFrame\n",
    "current_id = 0\n",
    "id_chunks = id_chunks.apply(add_ids, axis=1) # Put ID: Prefix before each paragraph\n",
    "\n",
    "\n",
    "chunk_number = 0\n",
    "new_id_list = []\n",
    "i = 0\n",
    "\n",
    "word_count_aux = []\n",
    "current_iteration = 0\n",
    "\n",
    "while chunk_number < len(id_chunks)-5:\n",
    "    word_count = 0\n",
    "    i = 0\n",
    "    while word_count < 550 and i+chunk_number<len(id_chunks)-1:\n",
    "        i += 1\n",
    "        final_document = \"\\n\".join(f\"{id_chunks.at[k, 'Chunk']}\" for k in range(chunk_number, i + chunk_number))\n",
    "        word_count = count_words(final_document)\n",
    "    \n",
    "\n",
    "    if(i ==1):\n",
    "        final_document = \"\\n\".join(f\"{id_chunks.at[k, 'Chunk']}\" for k in range(chunk_number, i + chunk_number))\n",
    "    else:\n",
    "        final_document = \"\\n\".join(f\"{id_chunks.at[k, 'Chunk']}\" for k in range(chunk_number, i-1 + chunk_number))\n",
    "    \n",
    "    \n",
    "    question = f\"\\nDocument:\\n{final_document}\"\n",
    "\n",
    "    word_count = count_words(final_document)\n",
    "    word_count_aux.append(word_count)\n",
    "    chunk_number = chunk_number + i-1\n",
    "\n",
    "    prompt = system_prompt + question\n",
    "    gpt_output = LLM_prompt(model_type=\"ChatGPT\", user_prompt=prompt)\n",
    "\n",
    "\n",
    "    # For books where there is dubious content, Gemini refuses to run the prompt and returns mistake. This is to avoid being stalled here forever.\n",
    "    if gpt_output == \"content_flag_increment\":\n",
    "        chunk_number = chunk_number + 1\n",
    "\n",
    "    else:\n",
    "        pattern = r\"Answer: ID \\w+\"\n",
    "        match = re.search(pattern, gpt_output)\n",
    "\n",
    "        if match == None:\n",
    "            print(\"repeat this one\")\n",
    "        else:\n",
    "            gpt_output1 = match.group(0)\n",
    "            print(gpt_output1)\n",
    "            pattern = r'\\d+'\n",
    "            match = re.search(pattern, gpt_output1)\n",
    "            chunk_number = int(match.group())\n",
    "            new_id_list.append(chunk_number)\n",
    "            if(new_id_list[-1] == chunk_number):\n",
    "                chunk_number = chunk_number + 1\n",
    "\n",
    "\n",
    "new_id_list.append(len(id_chunks))\n",
    "\n",
    "# Remove IDs as they no longer make sense here.\n",
    "id_chunks['Chunk'] = id_chunks['Chunk'].str.replace(r'^ID \\d+:\\s*', '', regex=True)\n",
    "\n",
    "\n",
    "#Create final dataframe from chunks\n",
    "new_final_chunks = []\n",
    "chapter_chunk = []\n",
    "for i in range(len(new_id_list)):\n",
    "    # Calculate the start and end indices for slicing aux1\n",
    "    start_idx = new_id_list[i-1] if i > 0 else 0\n",
    "    end_idx = new_id_list[i]\n",
    "    new_final_chunks.append('\\n'.join(id_chunks.iloc[start_idx: end_idx, 0]))\n",
    "\n",
    "    #This is for chunks where Gemini thinks the text should be spanned between 2 different paragraphs\n",
    "    if(paragraph_chunks[\"Chapter\"][start_idx] != paragraph_chunks[\"Chapter\"][end_idx-1]):\n",
    "        chapter_chunk.append(f\"{paragraph_chunks['Chapter'][start_idx]} and {paragraph_chunks['Chapter'][end_idx-1]}\")\n",
    "    else:\n",
    "        chapter_chunk.append(paragraph_chunks['Chapter'][start_idx])\n",
    "\n",
    "# Write new Chunks Dataframe\n",
    "df_new_final_chunks = pd.DataFrame({'Chapter': chapter_chunk, 'Chunk': new_final_chunks})\n",
    "df_new_final_chunks.to_excel(fileOut, index=False)\n",
    "print(f\"{book_name} Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac6930534f871d314aca2610c2357ec063ba4065ca1d2a97333736987f270c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
