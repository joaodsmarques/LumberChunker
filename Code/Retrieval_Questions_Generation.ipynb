{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"Put Here API_KEY\")\n",
    "import tiktoken\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Function aims to do string processing on the Chunk_Must_Contain String. Namely, ChatGPT sometimes quotes a big sentence with '...' in the middle, which is undesirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(s):\n",
    "    # Check if s is a missing value (pd.NA or None)\n",
    "    if pd.isna(s):\n",
    "        return s  # Just return the missing value without change\n",
    "\n",
    "    # Find index of '...'\n",
    "    index = s.find('...')\n",
    "\n",
    "    # If '...' is not found, return the string as is\n",
    "    if index == -1:\n",
    "        return s\n",
    "\n",
    "    # If '...' is at the beginning, remove it and strip whitespace\n",
    "    if index == 0:\n",
    "        return s.replace('...', '', 1).strip()\n",
    "\n",
    "    # If '...' is after at least 5 words\n",
    "    words = s[:index].split()\n",
    "    if len(words) >= 5:\n",
    "        return ' '.join(words)\n",
    "\n",
    "    # If none of the above conditions are met, return the string as is\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT_prompt(chunk, book_name, author_name):\n",
    "    system_prompt = f\"\"\"Your task is to generate a question-answer pair that is specific to the provided text excerpts from the book \"{book_name}\" by {author_name}. The question should be unique to the passage, meaning it cannot be easily answered by other parts of the book.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Read the Passage: Carefully read the provided text excerpt from the book. Understand the context, key events, and specific details mentioned.\n",
    "\n",
    "Formulate a Question: Create a question that is:\n",
    "\n",
    "Directly related to the passage: The question should be based on the specific information or events described in the text.\n",
    "Unique to the passage: The question should not be answerable with information from other parts of the book.\n",
    "Type: Focus on creating a \"When/What/Where\" question to encourage specificity and conciseness.\n",
    "\n",
    "Provide a Concise Answer: Write an answer that is:\n",
    "\n",
    "Direct and informative: Limit the answer to a maximum of two sentences. Ensure it directly addresses the question and is supported by the passage.\n",
    "Self-contained: The answer should make sense on its own and should not require additional context from outside the passage.\n",
    "Cite the Supporting Passage: Include the passage that contains the information needed to answer the question. This will be used to verify the accuracy of the answer and the relevance of the question. Do not use '...'. The passage should be quoted without breaks.\n",
    "\n",
    "Example (from a different book):\n",
    "\n",
    "Passage: “Do you talk of the second sight, or deutero-scopia?” said the soldier; “I remember memorable Major Munro telling me how Murdoch Mackenzie, born in Assint, a private gentleman in a company, and a pretty soldier, foretold the death of Donald Tough, a Lochaber man, and certain other persons, as well as the hurt of the major himself at a sudden onfall at the siege of Trailsund.”\n",
    "Question: Where was Murdoch Mackenzie born?\n",
    "\n",
    "Answer: He was born in Assint.\n",
    "\n",
    "Supporting Passage: “I remember memorable Major Munro telling me how Murdoch Mackenzie, born in Assint”\n",
    "\"\"\"\n",
    "    user_prompt = f\"\"\"Consider now this new case:\\nPassage: {chunk}\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        temperature=0.3,\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_dataframe(df, min_iterations=80, max_iterations=120):\n",
    "    length = len(df)\n",
    "    \n",
    "    # Calculate the optimal step size to keep iterations within the desired range\n",
    "    if length < min_iterations:\n",
    "        step = 1  # If the dataframe is smaller than min_iterations, use a step of 1\n",
    "    else:\n",
    "        # Calculate step size to get as close as possible to max_iterations without exceeding it\n",
    "        step = length // max_iterations + (length % max_iterations > 0)\n",
    "        \n",
    "        # Adjust step to ensure the number of iterations doesn't fall below min_iterations\n",
    "        while length // step < min_iterations:\n",
    "            step -= 1\n",
    "\n",
    "    print(f\"DF length: {length}; DF step:{step}\")\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(\"hf://datasets/LumberChunker/GutenQA/GutenQA.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# Book Name Example for A Christmas Carol by Charles Dickens\n",
    "book_name = \"A_Christmas_Carol_-_Charles_Dickens\"\n",
    "book_chunks = dataset[dataset['Book Name'] == book_name].reset_index(drop=True)\n",
    "out_path = \"Choose Path to Write Output File\"\n",
    "file_path = f\"{out_path}/Gemini_Chunks_-_{book_name}.xlsx\"\n",
    "\n",
    "\n",
    "parts = book_name.split('_-_')\n",
    "book_name_extract = parts[0].replace('_', ' ')\n",
    "author_name_extract = parts[1].replace('_', ' ')\n",
    "\n",
    "print(f\"Starting book - {book_name_extract} by {author_name_extract}\")\n",
    "\n",
    "step = iterate_dataframe(book_chunks)\n",
    "\n",
    "pattern = r'Question: (.*?)\\n\\nAnswer: (.*?)\\n\\nSupporting Passage: \"(.*?)\"'\n",
    "# Lists to hold the extracted data\n",
    "questions = []\n",
    "answers = []\n",
    "supporting_passages = []\n",
    "chunk_list = []\n",
    "chunk_idx = []\n",
    "\n",
    "\n",
    "\n",
    "# Idea is to cover questions from different parts of the book and not just from the begining.\n",
    "for i in tqdm(range(0,len(book_chunks), step)): \n",
    "    chunk = book_chunks[\"Chunk\"][i]\n",
    "    answer = GPT_prompt(chunk, book_name_extract, author_name_extract)\n",
    "\n",
    "    match = re.search(pattern, answer)\n",
    "    if match:\n",
    "        if(len(match.group(1)) > 0 and len(match.group(2)) > 0 and len(match.group(3)) > 0):\n",
    "            questions.append(match.group(1))\n",
    "            answers.append(match.group(2))\n",
    "            supporting_passages.append(match.group(3))\n",
    "            chunk_list.append(chunk)\n",
    "            chunk_idx.append(i)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Initialize the new columns with default values (NaN or None can be used if no default value is preferred)\n",
    "book_chunks[\"Question\"] = pd.NA\n",
    "book_chunks[\"Answer\"] = pd.NA\n",
    "book_chunks[\"Chunk Must Contain\"] = pd.NA\n",
    "\n",
    "# Use the chunk_idx to assign data to the correct rows\n",
    "book_chunks.loc[chunk_idx, \"Question\"] = questions\n",
    "book_chunks.loc[chunk_idx, \"Answer\"] = answers\n",
    "book_chunks.loc[chunk_idx, \"Chunk Must Contain\"] = supporting_passages\n",
    "\n",
    "book_chunks['Chunk Must Contain'] = book_chunks['Chunk Must Contain'].apply(process_string)\n",
    "book_chunks.to_excel(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac6930534f871d314aca2610c2357ec063ba4065ca1d2a97333736987f270c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
